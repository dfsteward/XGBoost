{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revised-triple",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "The below overview is an implementation of **XGBoost** on the Iris dataset and includes hyperparameter tuning with **GridSearchCV**        \n",
    "## Problem Statement\n",
    "The intent of this notebook is to serve as code-along/self-study reference material, and is a combination of original work and a [towards data science](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7) article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-headquarters",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "\n",
    "* [Gradient Boosting Theory](#theory)\n",
    "* [Import Libraries](#import_libraries)\n",
    "* [Import Data](#import_data)\n",
    "* [Splitting the data into training and testing sets](#split_data)\n",
    "* [Restructure data into DMatrix](#DMatrix)\n",
    "* [Define Parameters](#param)\n",
    "* [Create and Train the Model](#train_model)\n",
    "* [Evaluation Metrics](#eval_metrics)\n",
    "* [Gridsearch](#grid_search)\n",
    "* [Predictions from Gridsearch Model](#gs_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-nancy",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"theory\"></a>\n",
    "## Gradient Boosting Theory\n",
    "\n",
    "**`Boosting`** is an ensemble technique that combines many models together, but rather than training all of the models in isolation of one another, **boosting trains models in succession**, with **each new model being trained to correct the errors made by the previous ones**. Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "**`Gradient Boosting`** is an approach where new models are trained to predict the errors of prior models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-shopper",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amateur-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-construction",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import_data\"></a>\n",
    "## Import Data\n",
    "\n",
    "For this project we are using a seaborn built in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modified-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-spectrum",
   "metadata": {},
   "source": [
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The three classes in the Iris dataset:\n",
    "\n",
    "    Iris-setosa (n=50)\n",
    "    Iris-versicolor (n=50)\n",
    "    Iris-virginica (n=50)\n",
    "\n",
    "The four features of the Iris dataset:\n",
    "\n",
    "    sepal length in cm\n",
    "    sepal width in cm\n",
    "    petal length in cm\n",
    "    petal width in cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dominant",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"split_data\"></a>\n",
    "## Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "laden-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "configured-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-hammer",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DMatrix\"></a>\n",
    "## Restructure data into DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "advised-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-produce",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"param\"></a>\n",
    "## Define Parameters\n",
    "\n",
    "Reference the offical documentation here [XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "little-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-performer",
   "metadata": {},
   "source": [
    "   **eta**: (aka 'learning rate') gives us a chance to prevent overfitting by reducing the weight of the predictions of the new \n",
    "    trees; eta will be multiplied by the residuals being adding to reduce their weight; this effectively reduces the complexity \n",
    "    of the overall model.  \n",
    "    **max_depth**: maximum depth of the decision trees being trained  \n",
    "    **objective**: the loss function being used  \n",
    "    **num_class**: the number of classes in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-contract",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"train_model\"></a>\n",
    "## Create and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "empirical-shakespeare",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:27:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-advocacy",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"eval_metrics\"></a>\n",
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tamil-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-broadcast",
   "metadata": {},
   "source": [
    "Fit the model to our Test data, then compare the predictions to the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indonesian-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "clean-easter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9333333333333332\n",
      "Recall = 0.9722222222222222\n",
      "Accuracy = 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "particular-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0  4  0]\n",
      " [ 0  1 11]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.80      1.00      0.89         4\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.93      0.97      0.95        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,best_preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,best_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-estonia",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"grid_search\"></a>\n",
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aggressive-papua",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:44:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\devin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=4,\n",
       "             param_grid={'colsample_bytree': [0.3, 0.4, 0.5, 0.7],\n",
       "                         'eta': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
       "                         'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                         'max_depth': [3, 4, 5, 6, 8, 10, 12, 15],\n",
       "                         'min_child_weight': [1, 3, 5, 7]},\n",
       "             scoring='neg_log_loss')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#create instance of model to use\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "#define parameters to test\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }\n",
    "\n",
    "#instantiate GridSearchCV with the model and parameters\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, \n",
    "                    n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "#fit the estimator to the training data\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-dispatch",
   "metadata": {},
   "source": [
    "Inspect the best parameters; best_params_  attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "synthetic-clear",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'eta': 0.3,\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-sunrise",
   "metadata": {},
   "source": [
    "Inspect the best estimator in the best\\_estimator_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nonprofit-steam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, eta=0.3, gamma=0.3,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-programming",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"gs_predict\"></a>\n",
    "## Predictions from Gridsearch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "clear-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "opened-unemployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from model with GridSearchCV parameters:\n",
      "\n",
      "\n",
      "[[14  0  0]\n",
      " [ 0  4  0]\n",
      " [ 0  2 10]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.67      1.00      0.80         4\n",
      "           2       1.00      0.83      0.91        12\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.89      0.94      0.90        30\n",
      "weighted avg       0.96      0.93      0.94        30\n",
      "\n",
      "\n",
      "\n",
      "Precision = 0.8888888888888888\n",
      "Recall = 0.9444444444444445\n",
      "Accuracy = 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Predictions from model with GridSearchCV parameters:')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,grid_predictions))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,grid_predictions))\n",
    "print('\\n')\n",
    "print(\"Precision = {}\".format(precision_score(y_test, grid_predictions, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, grid_predictions, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, grid_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "focal-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from model with manually defined parameters (above):\n",
      "\n",
      "\n",
      "[[14  0  0]\n",
      " [ 0  4  0]\n",
      " [ 0  1 11]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.80      1.00      0.89         4\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.93      0.97      0.95        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      "\n",
      "Precision = 0.9333333333333332\n",
      "Recall = 0.9722222222222222\n",
      "Accuracy = 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Predictions from model with manually defined parameters (above):')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,best_preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,best_preds))\n",
    "print('\\n')\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
